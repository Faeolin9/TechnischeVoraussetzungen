{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paho-mqtt in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (6.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from plotly) (2.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tedle\\anaconda3\\envs\\mlagents\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Ben√∂tigte Pakete installieren\n",
    "!pip install paho-mqtt pandas numpy scikit-learn matplotlib seaborn plotly\n",
    "\n",
    "import paho.mqtt.client as mqtt\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Importe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Plotting einrichten\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01matexit\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Start mosquitto broker on localhost:1883\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m mosq_proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmosquitto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1883\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-v\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -v = verbose logs in the cell\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstop_mosquitto\u001b[39m():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mosq_proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# still running\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tedle\\anaconda3\\envs\\mlagents\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\tedle\\anaconda3\\envs\\mlagents\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1472\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1473\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess, time, atexit, os, signal\n",
    "\n",
    "# Start mosquitto broker on localhost:1883\n",
    "mosq_proc = subprocess.Popen(\n",
    "    [\"mosquitto\", \"-p\", \"1883\", \"-v\"]  # -v = verbose logs in the cell\n",
    ")\n",
    "\n",
    "def stop_mosquitto():\n",
    "    if mosq_proc.poll() is None:  # still running\n",
    "        mosq_proc.terminate()\n",
    "        try:\n",
    "            mosq_proc.wait(timeout=5)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            mosq_proc.kill()\n",
    "\n",
    "atexit.register(stop_mosquitto)\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"Mosquitto started with PID\", mosq_proc.pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MQTT-Konfiguration\n",
    "MQTT_BROKER = \"127.0.0.1\"  # Selbstreferenzierende IP-Adresse\n",
    "MQTT_PORT = 1883\n",
    "MQTT_KEEPALIVE = 60\n",
    "\n",
    "# Themen f√ºr unsere Sensoren\n",
    "TOPICS = {\n",
    "    'temperature': 'tutorial/sensors/temperature',\n",
    "    'humidity': 'tutorial/sensors/humidity',\n",
    "    'pressure': 'tutorial/sensors/pressure',\n",
    "    'vibration': 'tutorial/sensors/vibration'\n",
    "}\n",
    "\n",
    "# Globale Datenspeicherung\n",
    "sensor_data = []\n",
    "data_lock = threading.Lock()\n",
    "\n",
    "print(f\"MQTT Broker: {MQTT_BROKER}:{MQTT_PORT}\")\n",
    "print(f\"Themen: {list(TOPICS.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQTTDataCollector:\n",
    "    def __init__(self, broker, port, topics):\n",
    "        self.broker = broker\n",
    "        self.port = port\n",
    "        self.topics = topics\n",
    "        self.client = mqtt.Client()\n",
    "        self.data = []\n",
    "        self.is_connected = False\n",
    "        \n",
    "        # Callbacks einrichten\n",
    "        self.client.on_connect = self.on_connect\n",
    "        self.client.on_message = self.on_message\n",
    "        self.client.on_disconnect = self.on_disconnect\n",
    "    \n",
    "    def on_connect(self, client, userdata, flags, rc):\n",
    "        if rc == 0:\n",
    "            print(\"Erfolgreich mit dem MQTT-Broker verbunden!\")\n",
    "            self.is_connected = True\n",
    "            # Alle Themen abonnieren\n",
    "            for topic_name, topic in self.topics.items():\n",
    "                client.subscribe(topic)\n",
    "                print(f\"Abonniert: {topic}\")\n",
    "        else:\n",
    "            print(f\"Verbindung zum MQTT-Broker fehlgeschlagen. R√ºckgabecode: {rc}\")\n",
    "    \n",
    "    def on_message(self, client, userdata, msg):\n",
    "        try:\n",
    "            # Nachricht dekodieren\n",
    "            topic = msg.topic\n",
    "            payload = json.loads(msg.payload.decode())\n",
    "            \n",
    "            # Zeitstempel hinzuf√ºgen\n",
    "            payload['timestamp'] = datetime.now().isoformat()\n",
    "            payload['topic'] = topic\n",
    "            \n",
    "            # Daten threadsicher speichern\n",
    "            with data_lock:\n",
    "                self.data.append(payload)\n",
    "                sensor_data.append(payload)\n",
    "            \n",
    "            print(f\"Empfangen: {topic} -> {payload}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei der Verarbeitung der Nachricht: {e}\")\n",
    "    \n",
    "    def on_disconnect(self, client, userdata, rc):\n",
    "        self.is_connected = False\n",
    "        print(\"Vom MQTT-Broker getrennt\")\n",
    "    \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.client.connect(self.broker, self.port, MQTT_KEEPALIVE)\n",
    "            self.client.loop_start()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Verbindungsfehler: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def disconnect(self):\n",
    "        self.client.loop_stop()\n",
    "        self.client.disconnect()\n",
    "    \n",
    "    def get_data_as_dataframe(self):\n",
    "        with data_lock:\n",
    "            if self.data:\n",
    "                return pd.DataFrame(self.data)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "# Collector-Instanz erstellen\n",
    "collector = MQTTDataCollector(MQTT_BROKER, MQTT_PORT, TOPICS)\n",
    "print(\"MQTT Data Collector initialisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MQTT_BROKER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Publisher-Instanz erstellen\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m publisher \u001b[38;5;241m=\u001b[39m MockSensorPublisher(\u001b[43mMQTT_BROKER\u001b[49m, MQTT_PORT, TOPICS)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMock Sensor Publisher initialisiert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MQTT_BROKER' is not defined"
     ]
    }
   ],
   "source": [
    "class MockSensorPublisher:\n",
    "    def __init__(self, broker, port, topics):\n",
    "        self.broker = broker\n",
    "        self.port = port\n",
    "        self.topics = topics\n",
    "        self.client = mqtt.Client()\n",
    "        self.publishing = False\n",
    "        \n",
    "        # Sensorparameter f√ºr realistische Daten\n",
    "        self.sensor_params = {\n",
    "            'temperature': {'mean': 22.0, 'std': 3.0, 'min': 15, 'max': 35},\n",
    "            'humidity': {'mean': 45.0, 'std': 10.0, 'min': 20, 'max': 80},\n",
    "            'pressure': {'mean': 1013.25, 'std': 5.0, 'min': 990, 'max': 1040},\n",
    "            'vibration': {'mean': 0.1, 'std': 0.05, 'min': 0, 'max': 1.0}\n",
    "        }\n",
    "    \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.client.connect(self.broker, self.port, MQTT_KEEPALIVE)\n",
    "            self.client.loop_start()\n",
    "            print(\"Publisher mit MQTT-Broker verbunden\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Publisher-Verbindungsfehler: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_sensor_reading(self, sensor_type):\n",
    "        \"\"\"Realistische Sensormesswerte mit einigen Anomalien generieren\"\"\"\n",
    "        params = self.sensor_params[sensor_type]\n",
    "        \n",
    "        # 95% normale Messwerte, 5% Anomalien\n",
    "        if np.random.random() < 0.95:\n",
    "            # Normaler Messwert\n",
    "            value = np.random.normal(params['mean'], params['std'])\n",
    "            anomaly = False\n",
    "        else:\n",
    "            # Anomaler Messwert\n",
    "            if np.random.random() < 0.5:\n",
    "                value = params['min'] - np.random.uniform(0, 5)  # Unter dem Normalwert\n",
    "            else:\n",
    "                value = params['max'] + np.random.uniform(0, 10)  # √úber dem Normalwert\n",
    "            anomaly = True\n",
    "        \n",
    "        # Auf vern√ºnftige Grenzen beschr√§nken\n",
    "        value = np.clip(value, params['min'] - 10, params['max'] + 15)\n",
    "        \n",
    "        return {\n",
    "            'sensor_type': sensor_type,\n",
    "            'value': round(value, 2),\n",
    "            'unit': self.get_unit(sensor_type),\n",
    "            'device_id': f\"{sensor_type}_sensor_001\",\n",
    "            'anomaly': anomaly,\n",
    "            'reading_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def get_unit(self, sensor_type):\n",
    "        units = {\n",
    "            'temperature': '¬∞C',\n",
    "            'humidity': '%',\n",
    "            'pressure': 'hPa',\n",
    "            'vibration': 'g'\n",
    "        }\n",
    "        return units.get(sensor_type, '')\n",
    "    \n",
    "    def publish_sensor_data(self, duration_seconds=60, interval_seconds=2):\n",
    "        \"\"\"Sensordaten f√ºr die angegebene Dauer ver√∂ffentlichen\"\"\"\n",
    "        self.publishing = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"Beginne mit der Ver√∂ffentlichung von Sensordaten f√ºr {duration_seconds} Sekunden...\")\n",
    "        \n",
    "        while self.publishing and (time.time() - start_time) < duration_seconds:\n",
    "            for sensor_type, topic in self.topics.items():\n",
    "                reading = self.generate_sensor_reading(sensor_type)\n",
    "                \n",
    "                # An MQTT ver√∂ffentlichen\n",
    "                self.client.publish(topic, json.dumps(reading))\n",
    "                print(f\"Ver√∂ffentlicht {sensor_type}: {reading['value']} {reading['unit']}\")\n",
    "            \n",
    "            time.sleep(interval_seconds)\n",
    "        \n",
    "        print(\"Ver√∂ffentlichung der Sensordaten beendet\")\n",
    "        self.publishing = False\n",
    "    \n",
    "    def stop_publishing(self):\n",
    "        self.publishing = False\n",
    "    \n",
    "    def disconnect(self):\n",
    "        self.client.loop_stop()\n",
    "        self.client.disconnect()\n",
    "\n",
    "# Publisher-Instanz erstellen\n",
    "publisher = MockSensorPublisher(MQTT_BROKER, MQTT_PORT, TOPICS)\n",
    "print(\"Mock Sensor Publisher initialisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbinde mit MQTT-Broker...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerbinde mit MQTT-Broker...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Zuerst den Collector verbinden\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcollector\u001b[49m\u001b[38;5;241m.\u001b[39mconnect():\n\u001b[0;32m      6\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Warten, bis sich die Verbindung stabilisiert hat\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Publisher verbinden\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collector' is not defined"
     ]
    }
   ],
   "source": [
    "# Publisher und Collector verbinden\n",
    "print(\"Verbinde mit MQTT-Broker...\")\n",
    "\n",
    "# Zuerst den Collector verbinden\n",
    "if collector.connect():\n",
    "    time.sleep(2)  # Warten, bis sich die Verbindung stabilisiert hat\n",
    "    \n",
    "    # Publisher verbinden\n",
    "    if publisher.connect():\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Datenver√∂ffentlichung in einem separaten Thread starten\n",
    "        def publish_data():\n",
    "            publisher.publish_sensor_data(duration_seconds=5, interval_seconds=1)\n",
    "        \n",
    "        publish_thread = threading.Thread(target=publish_data)\n",
    "        publish_thread.start()\n",
    "        \n",
    "        print(\"\\nDatenerfassung gestartet. Sammle f√ºr 5 Sekunden...\")\n",
    "        print(\"Beobachten Sie die Echtzeitdaten unten:\")\n",
    "        \n",
    "        # Warten, bis die Ver√∂ffentlichung abgeschlossen ist\n",
    "        publish_thread.join()\n",
    "        \n",
    "        # Einen Moment f√ºr die letzten Nachrichten geben\n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(\"\\nDatenerfassung abgeschlossen!\")\n",
    "    else:\n",
    "        print(\"Verbindung des Publishers fehlgeschlagen\")\n",
    "else:\n",
    "    print(\"Verbindung des Collectors fehlgeschlagen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesammelte Daten als DataFrame abrufen\n",
    "df = collector.get_data_as_dataframe()\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"Gesammelt: {len(df)} Sensormesswerte\")\n",
    "    print(f\"Datenform: {df.shape}\")\n",
    "    print(\"\\nErste paar Datens√§tze:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nDatentypen:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nVerteilung der Sensortypen:\")\n",
    "    print(df['sensor_type'].value_counts())\n",
    "else:\n",
    "    print(\"Keine Daten gesammelt. Stellen Sie sicher, dass die MQTT-Verbindung erfolgreich war.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenvorverarbeitung und Visualisierung\n",
    "if not df.empty:\n",
    "    # Zeitstempel in datetime umwandeln\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['reading_time'] = pd.to_datetime(df['reading_time'])\n",
    "    \n",
    "    # Visualisierungen erstellen\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Echtzeit-Sensordaten-Visualisierung', fontsize=16)\n",
    "    \n",
    "    sensor_types = df['sensor_type'].unique()\n",
    "    \n",
    "    for i, sensor_type in enumerate(sensor_types[:4]):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        sensor_data_subset = df[df['sensor_type'] == sensor_type]\n",
    "        \n",
    "        # Zeitreihen-Plot\n",
    "        axes[row, col].plot(sensor_data_subset['timestamp'], \n",
    "                           sensor_data_subset['value'], \n",
    "                           marker='o', linewidth=2, markersize=4)\n",
    "        axes[row, col].set_title(f'{sensor_type.title()}-Messwerte')\n",
    "        axes[row, col].set_xlabel('Zeit')\n",
    "        axes[row, col].set_ylabel(f'Wert ({sensor_data_subset.iloc[0][\"unit\"]})')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Anomalien hervorheben\n",
    "        anomalies = sensor_data_subset[sensor_data_subset['anomaly'] == True]\n",
    "        if not anomalies.empty:\n",
    "            axes[row, col].scatter(anomalies['timestamp'], \n",
    "                                 anomalies['value'], \n",
    "                                 color='red', s=50, alpha=0.7, \n",
    "                                 label='Anomalien')\n",
    "            axes[row, col].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zusammenfassende Statistiken\n",
    "    print(\"\\nZusammenfassende Statistiken nach Sensortyp:\")\n",
    "    summary_stats = df.groupby('sensor_type')['value'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Zusammenfassung der Anomalieerkennung\n",
    "    print(\"\\nZusammenfassung der Anomalieerkennung:\")\n",
    "    anomaly_summary = df.groupby('sensor_type')['anomaly'].agg([\n",
    "        'sum', 'count', lambda x: (x.sum() / len(x) * 100)\n",
    "    ])\n",
    "    anomaly_summary.columns = ['Anomalien', 'Gesamtmesswerte', 'Anomalie_Prozentsatz']\n",
    "    anomaly_summary['Anomalie_Prozentsatz'] = anomaly_summary['Anomalie_Prozentsatz'].round(2)\n",
    "    print(anomaly_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorMLPipeline:\n",
    "    def __init__(self):\n",
    "        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)\n",
    "        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Merkmale f√ºr das maschinelle Lernen vorbereiten\"\"\"\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Merkmalsmatrix erstellen\n",
    "        features_df = df.copy()\n",
    "        \n",
    "        # Zeitbasierte Merkmale\n",
    "        features_df['hour'] = pd.to_datetime(features_df['timestamp']).dt.hour\n",
    "        features_df['minute'] = pd.to_datetime(features_df['timestamp']).dt.minute\n",
    "        \n",
    "        # Gleitende Statistiken (wenn gen√ºgend Daten vorhanden sind)\n",
    "        if len(features_df) > 5:\n",
    "            features_df = features_df.sort_values('timestamp')\n",
    "            features_df['rolling_mean_3'] = features_df.groupby('sensor_type')['value'].rolling(3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "            features_df['rolling_std_3'] = features_df.groupby('sensor_type')['value'].rolling(3, min_periods=1).std().fillna(0).reset_index(0, drop=True)\n",
    "        else:\n",
    "            features_df['rolling_mean_3'] = features_df['value']\n",
    "            features_df['rolling_std_3'] = 0\n",
    "        \n",
    "        # Verz√∂gerungsmerkmale (Lag-Features)\n",
    "        features_df['value_lag1'] = features_df.groupby('sensor_type')['value'].shift(1).fillna(features_df['value'])\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def train_anomaly_detector(self, df):\n",
    "        \"\"\"Anomalieerkennungsmodell trainieren\"\"\"\n",
    "        features_df = self.prepare_features(df)\n",
    "        \n",
    "        if features_df.empty:\n",
    "            print(\"Keine Daten zum Trainieren verf√ºgbar\")\n",
    "            return\n",
    "        \n",
    "        # Merkmale f√ºr die Anomalieerkennung ausw√§hlen\n",
    "        feature_cols = ['value', 'hour', 'minute', 'rolling_mean_3', 'rolling_std_3', 'value_lag1']\n",
    "        X = features_df[feature_cols].fillna(0)\n",
    "        \n",
    "        # Merkmale skalieren\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Anomalieerkennungsmodell trainieren\n",
    "        self.anomaly_detector.fit(X_scaled)\n",
    "        \n",
    "        print(\"Anomalieerkennungsmodell erfolgreich trainiert\")\n",
    "        return X_scaled\n",
    "    \n",
    "    def train_classifier(self, df):\n",
    "        \"\"\"Sensortyp-Klassifikator trainieren\"\"\"\n",
    "        features_df = self.prepare_features(df)\n",
    "        \n",
    "        if features_df.empty:\n",
    "            print(\"Keine Daten zum Trainieren verf√ºgbar\")\n",
    "            return\n",
    "        \n",
    "        # Merkmale f√ºr die Klassifizierung ausw√§hlen\n",
    "        feature_cols = ['value', 'hour', 'minute', 'rolling_mean_3', 'rolling_std_3', 'value_lag1']\n",
    "        X = features_df[feature_cols].fillna(0)\n",
    "        y = features_df['sensor_type']\n",
    "        \n",
    "        # Daten aufteilen\n",
    "        if len(X) > 10:  # Nur aufteilen, wenn gen√ºgend Daten vorhanden sind\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = X, X, y, y\n",
    "        \n",
    "        # Klassifikator trainieren\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluieren\n",
    "        y_pred = self.classifier.predict(X_test)\n",
    "        \n",
    "        print(\"Sensortyp-Klassifikator erfolgreich trainiert\")\n",
    "        print(\"\\nKlassifizierungsbericht:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return X_train, X_test, y_train, y_test, y_pred\n",
    "    \n",
    "    def detect_anomalies(self, df):\n",
    "        \"\"\"Anomalien in neuen Daten erkennen\"\"\"\n",
    "        features_df = self.prepare_features(df)\n",
    "        \n",
    "        if features_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        feature_cols = ['value', 'hour', 'minute', 'rolling_mean_3', 'rolling_std_3', 'value_lag1']\n",
    "        X = features_df[feature_cols].fillna(0)\n",
    "        \n",
    "        # Merkmale skalieren\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Anomalien vorhersagen (-1 = Anomalie, 1 = normal)\n",
    "        anomaly_predictions = self.anomaly_detector.predict(X_scaled)\n",
    "        anomaly_scores = self.anomaly_detector.score_samples(X_scaled)\n",
    "        \n",
    "        features_df['predicted_anomaly'] = anomaly_predictions == -1\n",
    "        features_df['anomaly_score'] = anomaly_scores\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def predict_sensor_type(self, df):\n",
    "        \"\"\"Sensortyp f√ºr neue Daten vorhersagen\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Modell noch nicht trainiert\")\n",
    "            return None\n",
    "        \n",
    "        features_df = self.prepare_features(df)\n",
    "        \n",
    "        if features_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        feature_cols = ['value', 'hour', 'minute', 'rolling_mean_3', 'rolling_std_3', 'value_lag1']\n",
    "        X = features_df[feature_cols].fillna(0)\n",
    "        \n",
    "        # Sensortypen vorhersagen\n",
    "        predictions = self.classifier.predict(X)\n",
    "        probabilities = self.classifier.predict_proba(X)\n",
    "        \n",
    "        features_df['predicted_sensor_type'] = predictions\n",
    "        \n",
    "        return features_df\n",
    "\n",
    "# ML-Pipeline initialisieren\n",
    "ml_pipeline = SensorMLPipeline()\n",
    "print(\"Machine-Learning-Pipeline initialisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelle trainieren, wenn Daten vorhanden sind\n",
    "if not df.empty:\n",
    "    print(\"Trainiere Anomalieerkennungsmodell...\")\n",
    "    ml_pipeline.train_anomaly_detector(df)\n",
    "    \n",
    "    print(\"\\nTrainiere Sensortyp-Klassifikator...\")\n",
    "    ml_pipeline.train_classifier(df)\n",
    "    \n",
    "    print(\"\\nModelle erfolgreich trainiert!\")\n",
    "else:\n",
    "    print(\"Keine Daten zum Trainieren verf√ºgbar. Bitte f√ºhren Sie zuerst den Abschnitt zur Datenerfassung aus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalieerkennung demonstrieren\n",
    "if not df.empty and ml_pipeline.is_trained:\n",
    "    print(\"F√ºhre Anomalieerkennung auf gesammelten Daten durch...\")\n",
    "    \n",
    "    # Anomalien erkennen\n",
    "    anomaly_results = ml_pipeline.detect_anomalies(df)\n",
    "    \n",
    "    # Echte vs. vorhergesagte Anomalien vergleichen\n",
    "    comparison = pd.DataFrame({\n",
    "        'sensor_type': anomaly_results['sensor_type'],\n",
    "        'value': anomaly_results['value'],\n",
    "        'true_anomaly': anomaly_results['anomaly'],\n",
    "        'predicted_anomaly': anomaly_results['predicted_anomaly'],\n",
    "        'anomaly_score': anomaly_results['anomaly_score']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nErgebnisse der Anomalieerkennung:\")\n",
    "    print(comparison.head(10))\n",
    "    \n",
    "    # Genauigkeit berechnen\n",
    "    accuracy = (comparison['true_anomaly'] == comparison['predicted_anomaly']).mean()\n",
    "    print(f\"\\nGenauigkeit der Anomalieerkennung: {accuracy:.2%}\")\n",
    "    \n",
    "    # Ergebnisse der Anomalieerkennung visualisieren\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Ergebnisse der Anomalieerkennung nach Sensortyp', fontsize=16)\n",
    "    \n",
    "    for i, sensor_type in enumerate(anomaly_results['sensor_type'].unique()[:4]):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        sensor_subset = anomaly_results[anomaly_results['sensor_type'] == sensor_type]\n",
    "        \n",
    "        # Normale Punkte plotten\n",
    "        normal_points = sensor_subset[~sensor_subset['predicted_anomaly']]\n",
    "        axes[row, col].scatter(range(len(normal_points)), normal_points['value'], \n",
    "                              c='blue', alpha=0.6, label='Normal')\n",
    "        \n",
    "        # Anomalien plotten\n",
    "        anomaly_points = sensor_subset[sensor_subset['predicted_anomaly']]\n",
    "        if not anomaly_points.empty:\n",
    "            axes[row, col].scatter(range(len(anomaly_points)), anomaly_points['value'], \n",
    "                                  c='red', alpha=0.8, label='Anomalie', s=60)\n",
    "        \n",
    "        axes[row, col].set_title(f'{sensor_type.title()} Anomalieerkennung')\n",
    "        axes[row, col].set_xlabel('Messungsindex')\n",
    "        axes[row, col].set_ylabel('Sensorwert')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeMQTTMLSystem:\n",
    "    def __init__(self, ml_pipeline, mqtt_collector):\n",
    "        self.ml_pipeline = ml_pipeline\n",
    "        self.mqtt_collector = mqtt_collector\n",
    "        self.predictions = []\n",
    "        self.running = False\n",
    "    \n",
    "    def process_real_time_data(self, duration_seconds=30):\n",
    "        \"\"\"Eingehende MQTT-Daten in Echtzeit verarbeiten\"\"\"\n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"Starte Echtzeitverarbeitung f√ºr {duration_seconds} Sekunden...\")\n",
    "        \n",
    "        while self.running and (time.time() - start_time) < duration_seconds:\n",
    "            # Neueste Daten abrufen\n",
    "            current_data = self.mqtt_collector.get_data_as_dataframe()\n",
    "            \n",
    "            if not current_data.empty and len(current_data) > 0:\n",
    "                # Die aktuellsten Messwerte abrufen\n",
    "                recent_data = current_data.tail(4)  # Letzte 4 Messwerte\n",
    "                \n",
    "                if self.ml_pipeline.is_trained:\n",
    "                    # Anomalien erkennen\n",
    "                    anomaly_results = self.ml_pipeline.detect_anomalies(recent_data)\n",
    "                    \n",
    "                    # Jede aktuelle Messung verarbeiten\n",
    "                    for _, row in anomaly_results.iterrows():\n",
    "                        prediction = {\n",
    "                            'timestamp': row['timestamp'],\n",
    "                            'sensor_type': row['sensor_type'],\n",
    "                            'value': row['value'],\n",
    "                            'predicted_anomaly': row['predicted_anomaly'],\n",
    "                            'anomaly_score': row['anomaly_score']\n",
    "                        }\n",
    "                        \n",
    "                        self.predictions.append(prediction)\n",
    "                        \n",
    "                        # Alarmieren, wenn eine Anomalie erkannt wird\n",
    "                        if row['predicted_anomaly']:\n",
    "                            print(f\"üö® ANOMALIE ERKANNT: {row['sensor_type']} = {row['value']} (Score: {row['anomaly_score']:.3f})\")\n",
    "                        else:\n",
    "                            print(f\"‚úÖ Normal: {row['sensor_type']} = {row['value']}\")\n",
    "            \n",
    "            time.sleep(2)  # Alle 2 Sekunden pr√ºfen\n",
    "        \n",
    "        self.running = False\n",
    "        print(\"\\nEchtzeitverarbeitung abgeschlossen!\")\n",
    "    \n",
    "    def get_predictions_summary(self):\n",
    "        \"\"\"Zusammenfassung der getroffenen Vorhersagen abrufen\"\"\"\n",
    "        if not self.predictions:\n",
    "            return \"Noch keine Vorhersagen getroffen\"\n",
    "        \n",
    "        pred_df = pd.DataFrame(self.predictions)\n",
    "        \n",
    "        summary = {\n",
    "            'total_predictions': len(pred_df),\n",
    "            'anomalies_detected': pred_df['predicted_anomaly'].sum(),\n",
    "            'anomaly_rate': pred_df['predicted_anomaly'].mean() * 100,\n",
    "            'sensors_monitored': pred_df['sensor_type'].nunique()\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Echtzeitsystem erstellen\n",
    "if ml_pipeline.is_trained:\n",
    "    realtime_system = RealTimeMQTTMLSystem(ml_pipeline, collector)\n",
    "    print(\"Echtzeit-MQTT-ML-System initialisiert\")\n",
    "else:\n",
    "    print(\"Bitte trainieren Sie zuerst die ML-Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echtzeitverarbeitung demonstrieren\n",
    "if 'realtime_system' in locals() and ml_pipeline.is_trained:\n",
    "    print(\"Starte Echtzeit-Demo...\")\n",
    "    \n",
    "    # Neue Daten im Hintergrund ver√∂ffentlichen\n",
    "    def publish_demo_data():\n",
    "        publisher.publish_sensor_data(duration_seconds=35, interval_seconds=1.5)\n",
    "    \n",
    "    # Echtzeitverarbeitung im Hintergrund starten\n",
    "    def process_demo_data():\n",
    "        realtime_system.process_real_time_data(duration_seconds=30)\n",
    "    \n",
    "    # Beide parallel ausf√ºhren\n",
    "    publish_thread = threading.Thread(target=publish_demo_data)\n",
    "    process_thread = threading.Thread(target=process_demo_data)\n",
    "    \n",
    "    publish_thread.start()\n",
    "    time.sleep(1)  # Kleine Verz√∂gerung\n",
    "    process_thread.start()\n",
    "    \n",
    "    # Auf Abschluss warten\n",
    "    process_thread.join()\n",
    "    publish_thread.join()\n",
    "    \n",
    "    # Zusammenfassung anzeigen\n",
    "    summary = realtime_system.get_predictions_summary()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ZUSAMMENFASSUNG DER ECHTZEITVERARBEITUNG\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "else:\n",
    "    print(\"Echtzeitsystem nicht verf√ºgbar. Bitte f√ºhren Sie zuerst die vorherigen Abschnitte aus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. √úbungsaufgaben {#practice}\n",
    "\n",
    "Jetzt ist es Zeit zu √ºben! Hier sind einige √úbungen, um Ihr Verst√§ndnis zu testen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbungsaufgabe 1: Integration eines benutzerdefinierten Sensors\n",
    "\n",
    "**Aufgabe**: F√ºgen Sie einen neuen Sensortyp namens \"Licht\" hinzu, der die Lichtintensit√§t (0-1000 Lux) misst.\n",
    "\n",
    "**Anforderungen**:\n",
    "1. F√ºgen Sie den neuen Sensor zum `TOPICS`-W√∂rterbuch hinzu\n",
    "2. Aktualisieren Sie den `MockSensorPublisher`, um Lichtsensordaten zu generieren\n",
    "3. Sammeln Sie Daten f√ºr diesen neuen Sensor\n",
    "4. Trainieren Sie die ML-Modelle neu, um den neuen Sensortyp einzubeziehen\n",
    "\n",
    "**Starter-Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úbungsaufgabe 1 - L√∂sung\n",
    "\n",
    "# Schritt 1: Lichtsensor zu den Themen hinzuf√ºgen\n",
    "TOPICS_EXTENDED = TOPICS.copy()\n",
    "TOPICS_EXTENDED['light'] = 'tutorial/sensors/light'\n",
    "\n",
    "# Schritt 2: Sensorparameter aktualisieren\n",
    "# Wir erstellen eine neue Instanz und f√ºgen den Parameter manuell hinzu\n",
    "publisher_extended = MockSensorPublisher(MQTT_BROKER, MQTT_PORT, TOPICS_EXTENDED)\n",
    "publisher_extended.sensor_params['light'] = {'mean': 500.0, 'std': 100.0, 'min': 0, 'max': 1000}\n",
    "\n",
    "# Helper f√ºr Einheit patchen\n",
    "original_get_unit = publisher_extended.get_unit\n",
    "def get_unit_extended(sensor_type):\n",
    "    if sensor_type == 'light':\n",
    "        return 'Lux'\n",
    "    return original_get_unit(sensor_type)\n",
    "publisher_extended.get_unit = get_unit_extended\n",
    "\n",
    "# Schritt 3: Neuen Collector mit erweiterten Themen erstellen\n",
    "collector_extended = MQTTDataCollector(MQTT_BROKER, MQTT_PORT, TOPICS_EXTENDED)\n",
    "\n",
    "# Schritt 4: Daten sammeln und Modelle neu trainieren\n",
    "print(\"Verbinde erweiterte Komponenten...\")\n",
    "if collector_extended.connect():\n",
    "    time.sleep(1)\n",
    "    if publisher_extended.connect():\n",
    "        time.sleep(1)\n",
    "        \n",
    "        print(\"Sammle Daten f√ºr alle Sensoren inklusive Licht...\")\n",
    "        \n",
    "        # Thread f√ºr das Publishen\n",
    "        def publish_extended():\n",
    "            publisher_extended.publish_sensor_data(duration_seconds=10, interval_seconds=0.5)\n",
    "            \n",
    "        pub_thread = threading.Thread(target=publish_extended)\n",
    "        pub_thread.start()\n",
    "        pub_thread.join()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Daten abrufen\n",
    "        df_extended = collector_extended.get_data_as_dataframe()\n",
    "        \n",
    "        if not df_extended.empty:\n",
    "            print(f\"\\nGesammelte Daten: {len(df_extended)} Zeilen\")\n",
    "            print(\"Sensortypen:\", df_extended['sensor_type'].unique())\n",
    "            \n",
    "            # Modelle neu trainieren\n",
    "            print(\"\\nTrainiere Modelle mit neuen Daten...\")\n",
    "            ml_pipeline_extended = SensorMLPipeline()\n",
    "            ml_pipeline_extended.train_anomaly_detector(df_extended)\n",
    "            ml_pipeline_extended.train_classifier(df_extended)\n",
    "            \n",
    "            # Aufr√§umen\n",
    "            collector_extended.disconnect()\n",
    "            publisher_extended.disconnect()\n",
    "            print(\"\\n√úbungsaufgabe 1 erfolgreich abgeschlossen!\")\n",
    "        else:\n",
    "            print(\"Keine Daten gesammelt.\")\n",
    "    else:\n",
    "        print(\"Publisher Verbindung fehlgeschlagen\")\n",
    "else:\n",
    "    print(\"Collector Verbindung fehlgeschlagen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbungsaufgabe 2: Fortgeschrittene Anomalieerkennung\n",
    "\n",
    "**Aufgabe**: Implementieren Sie ein anspruchsvolleres Anomalieerkennungssystem, das Folgendes ber√ºcksichtigt:\n",
    "1. Korrelationen zwischen Sensoren (z. B. sind Temperatur und Luftfeuchtigkeit oft korreliert)\n",
    "2. Zeitbasierte Muster (z. B. variiert die Temperatur je nach Tageszeit)\n",
    "3. Saisonale Trends\n",
    "\n",
    "**Anforderungen**:\n",
    "1. Erstellen Sie Merkmale, die Sensorkorrelationen erfassen\n",
    "2. F√ºgen Sie zeitbasierte Merkmale hinzu (Stunde, Wochentag usw.)\n",
    "3. Implementieren Sie einen Anomalie-Detektor f√ºr mehrere Sensoren\n",
    "4. Vergleichen Sie die Leistung mit dem einfachen Anomalie-Detektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úbungsaufgabe 2 - L√∂sung\n",
    "\n",
    "class AdvancedAnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.model = IsolationForest(contamination=0.05, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def create_advanced_features(self, df):\n",
    "        \"\"\"Erstellen Sie fortgeschrittene Merkmale f√ºr die Anomalieerkennung\"\"\"\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df_processed = df.copy()\n",
    "        df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])\n",
    "        \n",
    "        # Pivotieren um Sensoren als Spalten zu haben (f√ºr Korrelationen)\n",
    "        # Da Zeitstempel leicht variieren k√∂nnen, runden wir auf Sekunden oder nutzen resample\n",
    "        df_pivot = df_processed.pivot_table(\n",
    "            index=pd.Grouper(key='timestamp', freq='2S'), # 2 Sekunden Fenster\n",
    "            columns='sensor_type',\n",
    "            values='value',\n",
    "            aggfunc='mean'\n",
    "        ).interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Wenn wir nicht genug Spalten haben (z.B. nur ein Sensor), abbrechen\n",
    "        if df_pivot.shape[1] < 2:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Feature Engineering auf dem Pivot-DataFrame\n",
    "        features = pd.DataFrame(index=df_pivot.index)\n",
    "        \n",
    "        # 1. Rohwerte\n",
    "        for col in df_pivot.columns:\n",
    "            features[f'{col}_val'] = df_pivot[col]\n",
    "            \n",
    "        # 2. Verh√§ltnisse/Korrelationen (vereinfacht als Verh√§ltnis)\n",
    "        if 'temperature' in df_pivot.columns and 'humidity' in df_pivot.columns:\n",
    "            features['temp_humid_ratio'] = df_pivot['temperature'] / (df_pivot['humidity'] + 1)\n",
    "            \n",
    "        # 3. Zeitbasierte Merkmale (Zyklisch)\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features.index.hour / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features.index.hour / 24)\n",
    "        \n",
    "        # 4. Gleitende Statistiken √ºber alle Sensoren (Systemzustand)\n",
    "        features['system_mean'] = df_pivot.mean(axis=1)\n",
    "        features['system_std'] = df_pivot.std(axis=1)\n",
    "        \n",
    "        return features.dropna()\n",
    "    \n",
    "    def train(self, df):\n",
    "        \"\"\"Trainieren Sie den fortgeschrittenen Anomalie-Detektor\"\"\"\n",
    "        X = self.create_advanced_features(df)\n",
    "        if X.empty:\n",
    "            print(\"Nicht gen√ºgend Daten f√ºr fortgeschrittene Merkmale.\")\n",
    "            return\n",
    "            \n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled)\n",
    "        self.is_trained = True\n",
    "        print(f\"Fortgeschrittenes Modell trainiert mit {X.shape[1]} Merkmalen.\")\n",
    "        return X\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Anomalien mit fortgeschrittenen Merkmalen vorhersagen\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Modell nicht trainiert.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        X = self.create_advanced_features(df)\n",
    "        if X.empty:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        scores = self.model.score_samples(X_scaled)\n",
    "        \n",
    "        results = X.copy()\n",
    "        results['is_anomaly'] = predictions == -1\n",
    "        results['anomaly_score'] = scores\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Testen der L√∂sung\n",
    "if 'df' in locals() and not df.empty:\n",
    "    adv_detector = AdvancedAnomalyDetector()\n",
    "    adv_detector.train(df)\n",
    "    results = adv_detector.predict(df)\n",
    "    if not results.empty:\n",
    "        print(f\"Anomalien gefunden: {results['is_anomaly'].sum()}\")\n",
    "        print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbungsaufgabe 3: Vorausschauendes Wartungssystem\n",
    "\n",
    "**Aufgabe**: Erstellen Sie ein vorausschauendes Wartungssystem, das:\n",
    "1. Vorhersagt, wann ein Sensor aufgrund seiner Messwerte ausfallen k√∂nnte\n",
    "2. Die verbleibende Nutzungsdauer (RUL) von Ger√§ten sch√§tzt\n",
    "3. Wartungswarnungen sendet, bevor Ausf√§lle auftreten\n",
    "\n",
    "**Anforderungen**:\n",
    "1. Erstellen Sie synthetische Ausfalldaten (simulieren Sie die Sensoralterung)\n",
    "2. Erstellen Sie ein Regressionsmodell zur Vorhersage der RUL\n",
    "3. Implementieren Sie Alarmschwellen\n",
    "4. Erstellen Sie ein Dashboard, das den Zustand der Ger√§te anzeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úbungsaufgabe 3 - L√∂sung\n",
    "\n",
    "class PredictiveMaintenanceSystem:\n",
    "    def __init__(self):\n",
    "        self.rul_model = None # In einem echten Szenario: LinearRegression oder √§hnliches\n",
    "        self.health_thresholds = {'warning': 80, 'critical': 50}\n",
    "    \n",
    "    def simulate_sensor_degradation(self, df):\n",
    "        \"\"\"Sensoralterung im Laufe der Zeit simulieren\"\"\"\n",
    "        df_deg = df.copy()\n",
    "        df_deg['timestamp'] = pd.to_datetime(df_deg['timestamp'])\n",
    "        \n",
    "        # Wir simulieren, dass Vibration √ºber die Zeit zunimmt (Verschlei√ü)\n",
    "        # Sortieren nach Zeit\n",
    "        df_deg = df_deg.sort_values('timestamp')\n",
    "        \n",
    "        # Nur f√ºr Vibration\n",
    "        mask = df_deg['sensor_type'] == 'vibration'\n",
    "        n_samples = mask.sum()\n",
    "        \n",
    "        if n_samples > 0:\n",
    "            # Linearer Anstieg (Drift) + Zuf√§lliges Rauschen\n",
    "            drift = np.linspace(0, 0.5, n_samples) # Drift von 0 bis 0.5g\n",
    "            df_deg.loc[mask, 'value'] += drift\n",
    "            \n",
    "            # Rauschen nimmt auch zu\n",
    "            noise_scale = np.linspace(1, 3, n_samples)\n",
    "            noise = np.random.normal(0, 0.05, n_samples) * noise_scale\n",
    "            df_deg.loc[mask, 'value'] += noise\n",
    "            \n",
    "        return df_deg\n",
    "    \n",
    "    def calculate_health_score(self, df):\n",
    "        \"\"\"Ger√§tezustandsbewertung (0-100) berechnen\"\"\"\n",
    "        # Einfache Logik: Je h√∂her die Vibration, desto schlechter der Zustand\n",
    "        # Normal: 0.1g -> 100%, Kritisch: > 0.8g -> <50%\n",
    "        \n",
    "        health_scores = []\n",
    "        \n",
    "        # Gruppieren nach Zeitfenstern (z.B. letzte 10 Messungen)\n",
    "        # Hier vereinfacht: Score pro Messung\n",
    "        for _, row in df.iterrows():\n",
    "            score = 100\n",
    "            if row['sensor_type'] == 'vibration':\n",
    "                # Basiswert abziehen (0.1 ist normal)\n",
    "                degradation = max(0, row['value'] - 0.1)\n",
    "                # Skalierung: bei 1.0g ist Score 0\n",
    "                score = max(0, 100 - (degradation * 100))\n",
    "            elif row['sensor_type'] == 'temperature':\n",
    "                # Abweichung von 22 Grad\n",
    "                diff = abs(row['value'] - 22.0)\n",
    "                score = max(0, 100 - (diff * 2))\n",
    "                \n",
    "            health_scores.append(score)\n",
    "            \n",
    "        return pd.Series(health_scores, index=df.index)\n",
    "    \n",
    "    def predict_remaining_life(self, current_health, degradation_rate=0.1):\n",
    "        \"\"\"Verbleibende Nutzungsdauer in Tagen vorhersagen\"\"\"\n",
    "        # RUL = (Current Health - Critical Threshold) / Degradation Rate per Day\n",
    "        if current_health <= self.health_thresholds['critical']:\n",
    "            return 0\n",
    "        \n",
    "        rul = (current_health - self.health_thresholds['critical']) / degradation_rate\n",
    "        return rul\n",
    "    \n",
    "    def generate_maintenance_alerts(self, health_scores):\n",
    "        \"\"\"Wartungswarnungen basierend auf Vorhersagen generieren\"\"\"\n",
    "        alerts = []\n",
    "        for i, score in enumerate(health_scores):\n",
    "            if score < self.health_thresholds['critical']:\n",
    "                alerts.append(f\"KRITISCH: Index {i}, Score {score:.1f} - Sofortige Wartung erforderlich!\")\n",
    "            elif score < self.health_thresholds['warning']:\n",
    "                alerts.append(f\"WARNUNG: Index {i}, Score {score:.1f} - Wartung planen.\")\n",
    "        return alerts\n",
    "\n",
    "# Testen\n",
    "if 'df' in locals() and not df.empty:\n",
    "    pms = PredictiveMaintenanceSystem()\n",
    "    \n",
    "    # 1. Daten mit Alterung simulieren\n",
    "    df_degraded = pms.simulate_sensor_degradation(df)\n",
    "    \n",
    "    # 2. Gesundheitszustand berechnen\n",
    "    health = pms.calculate_health_score(df_degraded)\n",
    "    df_degraded['health_score'] = health\n",
    "    \n",
    "    # 3. Alerts generieren (nur die letzten paar anzeigen)\n",
    "    alerts = pms.generate_maintenance_alerts(health.tail(20))\n",
    "    \n",
    "    print(\"Simulierte Alterung und Gesundheitszustand:\")\n",
    "    print(df_degraded[df_degraded['sensor_type'] == 'vibration'].tail())\n",
    "    \n",
    "    print(\"\\nGenerierte Alarme (Auszug):\")\n",
    "    for alert in alerts[:5]:\n",
    "        print(alert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √úbungsaufgabe 4: Edge-Computing-Integration\n",
    "\n",
    "**Aufgabe**: Simulieren Sie ein Edge-Computing-Szenario, in dem:\n",
    "1. Lokale Verarbeitung die Daten√ºbertragung reduziert\n",
    "2. Nur wichtige Ereignisse an die Cloud gesendet werden\n",
    "3. Modelle lokal ausgef√ºhrt werden und regelm√§√üig Updates aus der Cloud erhalten\n",
    "\n",
    "**Anforderungen**:\n",
    "1. Implementieren Sie eine Logik f√ºr lokale vs. Cloud-Verarbeitung\n",
    "2. Erstellen Sie eine Datenfilterung basierend auf der Wichtigkeit\n",
    "3. Simulieren Sie die Modell-Synchronisation\n",
    "4. Vergleichen Sie die Bandbreitennutzung mit/ohne Edge-Verarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úbungsaufgabe 4 - L√∂sung\n",
    "\n",
    "class EdgeComputingSystem:\n",
    "    def __init__(self, ml_pipeline):\n",
    "        self.local_model = ml_pipeline\n",
    "        self.bandwidth_usage = {'local_only': 0, 'cloud_full': 0}\n",
    "        self.cloud_storage = []\n",
    "        \n",
    "    def process_locally(self, sensor_data):\n",
    "        \"\"\"Daten lokal auf dem Edge-Ger√§t verarbeiten\"\"\"\n",
    "        # Wir simulieren die Gr√∂√üe der Daten: ca. 100 Bytes pro Datensatz\n",
    "        data_size = len(sensor_data) * 100 \n",
    "        self.bandwidth_usage['cloud_full'] += data_size\n",
    "        \n",
    "        # Lokal Anomalien erkennen\n",
    "        if self.local_model.is_trained:\n",
    "            results = self.local_model.detect_anomalies(sensor_data)\n",
    "            \n",
    "            # Filter: Nur Anomalien oder periodische \"Heartbeats\" senden\n",
    "            # Wir senden alle Anomalien und 10% der normalen Daten als Stichprobe\n",
    "            important_data = results[\n",
    "                (results['predicted_anomaly'] == True) | \n",
    "                (np.random.random(len(results)) < 0.1)\n",
    "            ]\n",
    "            \n",
    "            return important_data\n",
    "        \n",
    "        return sensor_data # Fallback: Alles senden\n",
    "    \n",
    "    def send_to_cloud(self, filtered_data):\n",
    "        \"\"\"Nur wichtige Daten an die Cloud senden\"\"\"\n",
    "        if filtered_data.empty:\n",
    "            return\n",
    "            \n",
    "        # Gr√∂√üe der gefilterten Daten\n",
    "        filtered_size = len(filtered_data) * 100\n",
    "        self.bandwidth_usage['local_only'] += filtered_size\n",
    "        \n",
    "        # \"Senden\" (Speichern)\n",
    "        self.cloud_storage.append(filtered_data)\n",
    "        print(f\"Gesendet: {len(filtered_data)} Datens√§tze an Cloud.\")\n",
    "    \n",
    "    def sync_models(self):\n",
    "        \"\"\"Lokales Modell mit Cloud-Updates synchronisieren\"\"\"\n",
    "        # Simulation: Modellparameter aktualisieren\n",
    "        print(\"Synchronisiere Modell mit Cloud...\")\n",
    "        time.sleep(0.5)\n",
    "        print(\"Modell aktualisiert.\")\n",
    "    \n",
    "    def calculate_bandwidth_savings(self):\n",
    "        \"\"\"Bandbreiteneinsparungen durch Edge-Verarbeitung berechnen\"\"\"\n",
    "        full = self.bandwidth_usage['cloud_full']\n",
    "        optimized = self.bandwidth_usage['local_only']\n",
    "        \n",
    "        if full == 0: return 0\n",
    "        \n",
    "        savings = (full - optimized) / full * 100\n",
    "        return {\n",
    "            'full_transmission_bytes': full,\n",
    "            'edge_filtered_bytes': optimized,\n",
    "            'savings_percent': savings\n",
    "        }\n",
    "\n",
    "# Testen\n",
    "if 'df' in locals() and not df.empty and 'ml_pipeline' in locals():\n",
    "    edge_system = EdgeComputingSystem(ml_pipeline)\n",
    "    \n",
    "    # Wir simulieren einen Datenstrom in Chunks\n",
    "    chunk_size = 10\n",
    "    print(\"Simuliere Edge-Verarbeitung...\")\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        \n",
    "        # 1. Lokal verarbeiten und filtern\n",
    "        filtered_chunk = edge_system.process_locally(chunk)\n",
    "        \n",
    "        # 2. An Cloud senden\n",
    "        edge_system.send_to_cloud(filtered_chunk)\n",
    "        \n",
    "    # 3. Auswertung\n",
    "    stats = edge_system.calculate_bandwidth_savings()\n",
    "    print(\"\\nEdge-Computing Auswertung:\")\n",
    "    print(f\"Volle √úbertragung: {stats['full_transmission_bytes']} Bytes\")\n",
    "    print(f\"Gefilterte √úbertragung: {stats['edge_filtered_bytes']} Bytes\")\n",
    "    print(f\"Bandbreiteneinsparung: {stats['savings_percent']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindungen aufr√§umen\n",
    "try:\n",
    "    collector.disconnect()\n",
    "    publisher.disconnect()\n",
    "    print(\"Erfolgreich vom MQTT-Broker getrennt\")\n",
    "except:\n",
    "    print(\"Aufr√§umen abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
